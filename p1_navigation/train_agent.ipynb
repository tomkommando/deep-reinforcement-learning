{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agent\n",
    "\n",
    "Using this notebook you can:\n",
    "1. train an agent to map states to action values and plot the training process\n",
    "2. watch a trained agent to navigate the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages and declare constants\n",
    "If you want to watch trained agent playing, set `WEIGHTS_FILE = output/solution.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and set paths\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from dqn_agent import Agent     # import agent.py\n",
    "from dqn import dqn\n",
    "import pandas as pd\n",
    "\n",
    "# declare directory to save weights of trained network\n",
    "WEIGHTS_PATH = 'outputs/'\n",
    "# declare directory and filename of trained network weights\n",
    "WEIGHTS_FILE = None          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate Unity Environment\n",
    "If you have and existing weights file loaded and want to watch trained agent playing, initialise the agent with parameter: `no_graphics=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate environment make sure you replace the filename and path with one that matches your folder structure.\n",
    "env = UnityEnvironment(file_name=\"python/Banana.exe\", no_graphics=True)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]  # reset the environment\n",
    "state = env_info.vector_observations[0]  # get the current state\n",
    "state_size = len(state)\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialise Agent\n",
    "If you want to train the agent with Dueling network structure (see details in the [report](/Report.md)), initialise the agent with `duel=True` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing model, if you've got weights\n",
    "if WEIGHTS_FILE is not None:\n",
    "    qnetwork_weights = torch.load(WEIGHTS_FILE) \n",
    "else:\n",
    "    qnetwork_weights = None\n",
    "\n",
    "# Initialise agent. Set duel=True if you want dueling Q\n",
    "agent = Agent(state_size, action_size, seed=0, duel=True, qnetwork_weights=qnetwork_weights)  # initialise agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the agent\n",
    "If you have loaded existing weights and want to watch the trained agent playing. Set the `eps_start=0` and `eps_end=0` so the agent won't take random steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent\n",
    "scores, steps, yellow_bananas, blue_bananas, epsilons = dqn(env, agent, WEIGHTS_PATH, brain_name, n_episodes=2000, eps_start=1, eps_end=0.01, eps_decay=0.993)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put observations in lists\n",
    "columns = ['scores', 'steps', 'yellow_bananas', 'blue_bananas', 'epsilons']\n",
    "data = [scores, steps, yellow_bananas, blue_bananas, epsilons]\n",
    "\n",
    "# convert to dataframe\n",
    "df = pd.DataFrame(dict(zip(columns, data)))\n",
    "\n",
    "# calculate moving average\n",
    "df['Yellow Bananas Moving Avg 10'] = df['yellow_bananas'].rolling(window=10).mean()\n",
    "df['Blue Bananas Moving Avg 10'] = df['blue_bananas'].rolling(window=10).mean()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot banana collection\n",
    "plt.plot( 'Yellow Bananas Moving Avg 10', data=df, marker='', color='olive', linewidth=2)\n",
    "plt.plot( 'Blue Bananas Moving Avg 10', data=df, marker='', color='blue', linewidth=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores and epsilon\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# plot episode scores\n",
    "ax1.set_xlabel('Episodes')\n",
    "ax1.set_ylabel('Episode Scores', color='olive')\n",
    "ax1.plot(df['scores'], color='olive')\n",
    "ax1.tick_params(axis='y', labelcolor='olive')\n",
    "\n",
    "# instantiate a dual y-axis for epsilon. two plots share the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Epsilon', color='black')  \n",
    "ax2.plot(df['epsilons'], color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# set layout and show\n",
    "fig.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
