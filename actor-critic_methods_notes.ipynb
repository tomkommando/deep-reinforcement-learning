{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods\n",
    "\n",
    "The actor critic method is in cross section of `value-based` and `policy-based` methods. The goal is to use value based techniques to reduce variance and bias of policy based methods.\n",
    "\n",
    "Monte Carlo has `no bias`, but has `high variance`. Policy based methods give `low variance`, but is `biased. By combining these, we can make training faster and more stable.\n",
    "\n",
    "---\n",
    "\n",
    "Below some attempts to solve environments using DDPG:\n",
    "- [Pendulum](./ddpg-pendulum/DDPG.ipynb)\n",
    "- [Bipedal](./ddpg-bipedal/DDPG.ipynb)\n",
    "\n",
    "The above exercises are from [Udacity Deep Reinforcement Learning Git](https://github.com/udacity/deep-reinforcement-learning).\n",
    "\n",
    "Some extra lessons learnt: regulate exploration by adding some `noise` to policy weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C: Asynchronous Advantage Actor-Critic\n",
    "\n",
    "\n",
    "\n",
    "### N-step Bootstrapping\n",
    "\n",
    "TD is an `n` step bootstrap whereas Monte Carlo is `infinite` step bootstrap\n",
    "\n",
    "We often use 4-5 step TD and then Monte Carlo from then on\n",
    "\n",
    "### Parallel Training\n",
    "\n",
    "A3C doesn't use `replay buffer` like DQN methods. The reason we needed it in the first place is that the observations are collected sequentially and are thus correlated. By randomly sampling from the buffer, we can break the correlation.\n",
    "\n",
    "By running several environments and agents `in parallel`, each agent produce sequential gradients, however by sampling from different agents at the same time we can break the sequence since the agents are experiencing different things. This allows us to perform `on-policy` learning which is considered more stable than `off-policy` learning.\n",
    "\n",
    "\n",
    "## A2C: Advantage Actor-Critic\n",
    "\n",
    "Whereas in A3C all agents train their network independently and send their gradients `asynchronously` to the \"main\" network. This way all the agents have different weights they use to train their policy. In the A2C, every now and then, we `synchronise` the agents. That is, all the agents finish a segment of training, we combine take a copy of the \"main\" environment and  the weights and copy the main environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAE Generalised Advantage Estimation\n",
    "\n",
    "There is another way to estimate returns, called `Lambda return`. We often realise in simulations that bootstrapping with `n > 1` performs better than `n = 1`, but it's hard to know the optimal number of steps. Sometimes very small number is better, sometimes very large. So we introduce ${\\lambda}$, which we use to exponentially decay the value of different `n-step` weights. and then add them all up.\n",
    "\n",
    "![gae](./resources/gae.JPG)from Udacity lecture notes\n",
    "\n",
    "Then we just add up all the returns..\n",
    "- If ${\\lambda}=0$, we get only the One-step bootstrapping TD estimate\n",
    "- if ${\\lambda}=1$, we get the Infinite-step bootstrapping MC estimate.\n",
    "\n",
    "Combine this with A3C or A2C or any other policy-based method and it should make the training process faster and stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG: Deep Deterministic Policy Gradient for Continuous Action-space\n",
    "\n",
    "Updating algorithm is very similar to `DQN`. Researchers don't have concensus whether this should be classified as `DQN` or `Actor-critic` method. In the [paper](https://arxiv.org/abs/1509.02971) the method was first introduced, the guys call it `Actor-critic`. Nevertheless, the method is very useful. It looks to approximate a max value function of Q-value for the next step. For discrete action spaces, extimating the max is easy, but not so much in continuous action spaces.\n",
    "\n",
    "There are actor and critic, where actor learns ${argmax}_a{Q(s,a)}$, which is the best action. and the critic learns to evaluate the optimal action value function, by using the actor's best believed action.\n",
    "![ddpg](./resources/ddpg.JPG)from Udacity lecture notes\n",
    "\n",
    "---\n",
    "There are two uses, soft-updates to target networks and use of a replay buffer. Compared to legendary [DQN]() paper, which had two networks and the updates to the `target` network was carried out every 10000 steps, DDPG uses two copies of network weights, `actor` and `critic`, for both `regular` and `target`. Target network is updated using soft update like so:\n",
    "![ddpg update](./resources/ddpg-soft-update.JPG)from Udacity lecture notes\n",
    "\n",
    "\n",
    "Should converge faster. Soft updates can be used in any setting where there's regular and target network. Regulate with parameter ${\\tau}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- [Proximal Policy Algorithms](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "- [DeepRL](https://github.com/ShangtongZhang/DeepRL.git) algorithm implementations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
